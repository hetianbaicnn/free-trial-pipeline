{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import logging\n",
    "import sagemaker\n",
    "import stepfunctions\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import Parallel\n",
    "from stepfunctions.steps.sagemaker import TrainingStep, ModelStep, TransformStep\n",
    "from stepfunctions.workflow import Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define boto3 clients\n",
    "#s3_client = boto3.client('s3')\n",
    "\n",
    "# define catch-all execution role\n",
    "hbomax_datascience_service_role = 'arn:aws:iam::613630599026:role/hbomax-datascience-service-role'\n",
    "\n",
    "# set logging\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "# collect session info\n",
    "region = boto3.Session().region_name\n",
    "acount_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# define s3 bucket\n",
    "resources_bucket = 'hbomax-datascience-deployment-dev'\n",
    "resources_bucket2 = 'datascience-hbo-users'\n",
    "\n",
    "# sagemaker session\n",
    "sagemaker_session = sagemaker.Session(default_bucket=resources_bucket)\n",
    "\n",
    "# featurizer model name\n",
    "featurizer_model_name ='FTFeaturizer'\n",
    "\n",
    "# name the inference pipeline modek\n",
    "pipeline_model_name = 'FTPipeLineModel'\n",
    "\n",
    "# name the Stepfunctions pipeline\n",
    "pipeline_name = 'FTInferenceRoutine'\n",
    "\n",
    "# built-in XGBoost image\n",
    "xgboost_image = get_image_uri(region, 'xgboost', repo_version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data sources\n",
    "s3_train_data = f's3://{resources_bucket}/lifecycle/free-trial-propensity-model/train/raw'\n",
    "s3_test_data = f's3://{resources_bucket}/lifecycle/free-trial-propensity-model/test/raw'\n",
    "s3_new_data = f's3://{resources_bucket}/lifecycle/free-trial-propensity-model/new'\n",
    "\n",
    "s3_out_data = f's3://{resources_bucket}/lifecycle/free-trial-propensity-model/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define runtime input.  SageMaker expects unique names for each job, model and endpoint. \n",
    "execution_input = ExecutionInput(schema={\n",
    "    'SKLearnFeaturizerJobName': str,\n",
    "    'TransformTrainJobName': str,\n",
    "    'TransformTestJobName': str,\n",
    "    'FeaturizerModelName': str,\n",
    "    'XGBModelName': str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the SKLearn Preprocessing Estimator\n",
    "sklearn_featurizer = SKLearn(\n",
    "    entry_point='featurizer.py',\n",
    "    role=hbomax_datascience_service_role,\n",
    "    #role=sagemaker.get_execution_role(),\n",
    "    output_kms_key='alias/aws/s3',\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_featurizer_step = TrainingStep(\n",
    "    'Fit Featurizer', \n",
    "    estimator = sklearn_featurizer,\n",
    "    data={\n",
    "        'train': s3_train_data\n",
    "    },\n",
    "    #tags= {'task': 'tune','algorithm':'linlearner'},\n",
    "    job_name=execution_input['SKLearnFeaturizerJobName']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_featurizer_model_step = ModelStep(\n",
    "    'Create Featurizer Model', \n",
    "    model = fit_featurizer_step.get_expected_model(),\n",
    "    model_name=execution_input['FeaturizerModelName']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = Transformer(\n",
    "    model_name=featurizer_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.2xlarge',\n",
    "    strategy='SingleRecord',\n",
    "    assemble_with='Line',\n",
    "    #output_kms_key='alias/aws/s3',\n",
    "    output_path=f's3://{resources_bucket2}/lifecycle/free-trial-propensity-model/train/transformed'\n",
    ")\n",
    "\n",
    "transform_train_step = TransformStep(\n",
    "    'Transform Training Data',\n",
    "    transformer=train_transformer,\n",
    "    job_name=execution_input['TransformTrainJobName'],\n",
    "    model_name=featurizer_model_name,\n",
    "    data=s3_train_data,\n",
    "    content_type= 'text/csv',\n",
    "    split_type='Line',\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformer = Transformer(\n",
    "    model_name=featurizer_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.2xlarge',\n",
    "    strategy='SingleRecord',\n",
    "    assemble_with='Line',\n",
    "    #output_kms_key='alias/aws/s3',\n",
    "    output_path=f's3://{resources_bucket2}/lifecycle/free-trial-propensity-model/test/transformed'\n",
    ")\n",
    "\n",
    "transform_test_step = TransformStep(\n",
    "    'Transform Test Data',\n",
    "    transformer=test_transformer,\n",
    "    job_name=execution_input['TransformTestJobName'],\n",
    "    model_name=featurizer_model_name,\n",
    "    data=s3_test_data,\n",
    "    content_type= 'text/csv',\n",
    "    split_type='Line',\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_transform_step = Parallel(\n",
    "    state_id=\"Branch Transformations\"\n",
    ")\n",
    "\n",
    "parallel_transform_step.add_branch(transform_test_step)\n",
    "parallel_transform_step.add_branch(transform_train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the XGBoost Model Estimator\n",
    "xgboost_estimator = Estimator(image_name = xgboost_image,\n",
    "                          role = hbomax_datascience_service_role, \n",
    "                          train_instance_count = 1, \n",
    "                          train_instance_type='ml.m4.4xlarge',\n",
    "                          output_path = s3_out_data,\n",
    "                          output_kms_key = 'alias/aws/s3',\n",
    "                          hyperparameters = {\n",
    "                                             'eval_metric':'auc'\n",
    "                                            , 'alpha':1.218487609\n",
    "                                            , 'eta':0.225242353\n",
    "                                            , 'max_depth':10\n",
    "                                            , 'min_child_weight':2.284773815\n",
    "                                            , 'num_round':100\n",
    "                                            , 'objective':'binary:logistic'\n",
    "                                            , 'rate_drop':0.3\n",
    "                                            , 'tweedie_variance_power':1.4\n",
    "                                          },\n",
    "                          sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([fit_featurizer_step, create_featurizer_model_step, parallel_transform_step])\n",
    "$workflow_definition = steps.Chain([parallel_transform_step])\n",
    "\n",
    "workflow = Workflow(\n",
    "    name=pipeline_name,\n",
    "    definition=workflow_definition,\n",
    "    role=hbomax_datascience_service_role,\n",
    "    execution_input=execution_input\n",
    ")\n",
    "\n",
    "workflow.create()\n",
    "workflow.update(workflow_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
